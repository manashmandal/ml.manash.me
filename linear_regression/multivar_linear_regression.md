# মাল্টিভ্যারিয়েবল লিনিয়ার রিগ্রেশন

গত পর্বগুলোতে আমরা দেখেছিলাম সিঙ্গেল ভ্যারিয়েবল বিশিষ্ট সমস্যাগুলোতে কীভাবে লিনিয়ার মডেল ফিট করতে হয়। আজকে আমরা দেখব, সমস্যাটি যদি মাল্টি ভ্যারিয়েবল / কলাম / ফিচার বিশিষ্ট হয় তাহলে তার অ্যানালাইসিসটা কেমন হবে।

## মাল্টিভ্যারিয়েবল বিশিষ্ট ডেটাসেট

কাজ শুরুর আগে ডেটাসেটটা একনজর দেখা যাক,

| Size \( $$feet^{2} $$ \) | Number of Bedrooms | Number of floors | Age of home \(years\) | Price \($1000\) |
| --- | --- | --- | --- | --- |
| 2104 | 5 | 1 | 45 | 460 |
| 1416 | 3 | 2 | 40 | 232 |
| 1534 | 3 | 2 | 30 | 315 |
| 852 | 2 | 1 | 36 | 178 |

## লক্ষণীয়

লক্ষ করলে দেখা যাবে, আগের মত ইনপুট ভ্যারিয়েবল আর একটা নাই। বরং অনেকগুলো, তারমানে এখন আর আমরা ফিচার শুধু $$x$$ ধরলেই হবে না। এখন আমাদের প্রতিটা কলাম ম্যাথেমেটিক্যাল নোটেশন দিয়ে আলাদা করতে হবে যেন আমরা বুঝতে পারি কোনটা আসলে কোন কলাম। এটা করার জন্য আমরা প্রতি কলামের জন্য $$ x $$ এর সাবস্ক্রিপ্ট দিয়ে কলাম নাম্বার বসাব। সুপারস্ক্রিপ্টে রো \(Row\) ইন্ডেক্স বসবে এবং সাবস্ক্রিপ্টে বসবে কলাম \(Column\) ইন্ডেক্স।

**উদাহরণ: \(শুধু প্রথম Row এর জন্য\)**

$$Size \; ( feet^{2} ) = x_{1}^{(1)}$$

$$Number \; of \; bedrooms = x_{2}^{(1)}$$

$$Number \; of \; floors = x_{3}^{(1)}$$

$$Age \; of \; home = x_{4}^{(1)}$$

$$Price = y_{1}^{(1)}$$

তাহলে $$ i $$ তম ইনপুট ভ্যারিয়েবল হবে $$ x_{i} $$ এবং $$ i $$ তম আউটপুট ভ্যারিয়েবল হবে $$ y_{i} $$

**২য় উদাহরণ**

আমরা যদি দ্বিতীয় সারির ইনপুট ভ্যারিয়েবলগুলোকে ম্যাট্রিক্সে সাজাতে চাই তাহলে সেটা হবে এইরকম, যেহেতু আমরা নির্দিষ্ট কোন Columwise ভ্যারিয়েবল বিবেচনা করছি না, সবগুলো ভ্যারিয়েবল নিয়ে একটি ম্যাট্রিক্স তৈরি করেছি তাই আমাদের আলাদা করে সাবস্ক্রিপ্ট বসানোর মানে নেই।

$$

X^{(2)} = \begin{bmatrix}  1416 \\ 3 \\ 2 \\ 40  \end{bmatrix}

$$

এবং দ্বিতীয় সারির আউটপুট হবে,

$$

Y^{(2)} = \begin{bmatrix} 232 \end{bmatrix}

$$

আশা করি তাহলে তৃতীয় ও চতুর্থ সারির ম্যাট্রিক্স নোটেশন কী হবে বুঝতে পেরেছেন। নোটেশন বোঝা শেষ, এবার আমরা সরাসরি চলে যাব মডেল বিল্ডিংয়ে।

### হাইপোথিসিস \(Hypothesis\)

আগের হাইপোথিসিস ছিল এটা,

$$

h_{\theta}(x) = \theta_{0} + \theta_{1}x

$$

এটা দিয়ে আমাদের এই মাল্টি ভ্যারিয়েবল সেটে কাজ করবে না। তাহলে উপায়? হুঁ, উপায় আছে, সেটা হল প্রতিটা ভ্যারিয়েবলের আগে একটা করে নতুন প্যারামিটার গুণ করে দেওয়া।

$$

h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \theta_{4}x_{4} \; \dots (1)

$$

এখন আমরা থিটার বিভিন্ন মান ধরে ভালমন্দ প্রেডিকশন করতে পারব, যেমন,

$$

h_{\theta}(x) = 80 + 0.1x_{1} + 0.01x_{2} + 3x_{3} - 2x_{4} \; \dots (2)

$$

এই সমীকরণ $$ (2) $$ সিরিয়াসলি নেয়ার কিছু নাই, এটা চিন্তাভাবনাহীন উদাহরণ।

## আবারও গণিত

ভয়ের কিছু নেই, আমরা এখানে বেসিক ম্যাথেমেটিক্যাল নোটেশন নিয়েই আলোচনা করতে বসেছি। কারণ নোটেশনগুলো বুঝলে General Purpose Machine Learning এর থিওরি বুঝতে সমস্যা হবে না, আমিও শর্টকাটে লিখতে পারব, আপনিও বুঝতে পারবেন।

### হাইপোথিসিস মডিফিকেশন

আমরা সমীকরণ $$ (1) $$ এ মাল্টিভ্যারিয়েবল হাইপোথিসিস মডেলটা দেখতে পাচ্ছি। কথা হল, আমরা যদি সেটাকে ম্যাট্রিক্স আকারে সাজাতে চাই তাহলে বিশাল একটা সমস্যায় পড়ব। কারণ, হাইপোথিসিস এর প্যারামিটার শুরু হয়েছে $$ \theta_{0} $$ থেকে, কিন্তু ভ্যারিয়েবলের রো শুরু হয়েছে $$ x_{1} $$ থেকে। তারমানে মডেল প্যারামিটারের সংখ্যা কলামের সংখ্যার চেয়ে বেশি। ম্যাট্রিক্সের যোগ বিয়োগ করতে হলে ডাইমেনশন সমান হতে হয়, ম্যাট্রিক্স অপারেশনগুলো কার্যকর করার জন্য তাই আমরা সমীকরণ $$ (1) $$ কে একটু মডিফাই করব।

আমরা সমীকরণ $$ (1) $$ কে লিখতে পারি এভাবে, $$ h_{\theta}(x) = \theta_{0}x_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \theta_{4}x_{4} + \dots + \theta_{n}x_{n} \; \dots (3) $$

যদি আমরা $$ x_{0} = 1 $$ ধরি তাহলে সমীকরণ $$ (2) $$ এবং $$ (3) $$ এর মধ্যে পার্থক্য থাকবে না।

আমরা $$ X $$ ও $$ \theta $$ কে যদি $$ n $$ সংখ্যক ভ্যারিয়েবলের ম্যাট্রিক্সে রাখতে চাই তাহলে আমরা লিখবো এভাবে,

$$

X^{(i)} = \begin{bmatrix} x_{0} \\ x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix}

$$

একই ভাবে থিটা প্যারামিটারগুলোকে আমরা যদি ম্যাট্রিক্স আকারে লিখি তাহলে দেখাবে এরকম,

$$

\theta = \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \theta_{2} \\ \vdots \\ \theta_{n} \end{bmatrix}

$$

## কেন হাইপোথিসিস মডিফাই করা হল?

### ম্যাট্রিক্স মাল্টিপ্লিকেশন : রুল নাম্বার ১

দুইটা ম্যাট্রিক্স গুণ করার প্রথম শর্ত হল, প্রথম ম্যাট্রিক্সের কলাম সংখ্যা দ্বিতীয় ম্যাট্রিক্সের রো সংখ্যার সমান হতে হবে। আমরা যদি $$ x_{0} $$ না বসাতাম তাহলে দুইটার ডাইমেনশন কখনই সমান হত না। অবশ্য এখনও আমরা দ্বিতীয় ম্যাট্রিক্স অর্থাৎ, $$ \theta $$ কে ট্রান্সপোজ করি নাই, তাই একটু উলট পালট লাগতে পারে। ডাইমেনশন সমান করার আরেকটা সল্যুশন হতে পারত, আমরা যদি $$ \theta_{0} $$ উঠিয়ে দিতাম। কিন্তু প্যারামিটার উঠানো বুদ্ধিমানের কাজ নয়। আমাদের যদি একান্তই $$ \theta_{0} $$ না লাগে আমরা সেটার মান $$ 0 $$ বসিয়ে দিলেই হচ্ছে।

### ম্যাট্রিক্স মাল্টিপ্লিকেশন উদাহরণ:

লিনিয়ার অ্যালজেব্রা মনে না থাকলে এটা একটা সামান্য আইওয়াশ হিসেবে নিতে পারেন, নিচের সমীকরণে,

$$

Z = a_{1}x_{1} + a_{2}x_{2} + a_{3}x_{3}

$$

ধরি,

$$

A = \begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \end{bmatrix}

$$

এবং

$$

X = \begin{bmatrix} x_{1} & x_{2} & x_{3} \end{bmatrix}

$$

আমরা পুরো জিনিসটাকে তাহলে এভাবে ম্যাট্রিক্স আকারে লিখতে পারি,

$$

Z = A \times X

$$

তারমানে,

$$

A \times X = \begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \end{bmatrix} \times \begin{bmatrix} x_{1} & x_{2} & x_{3} \end{bmatrix} = a_{1}x_{1} + a_{2}x_{2} + a_{3}x_{3}

$$

### কিন্তু,

উদাহরণে, একটা কলাম ও আরেকটা রো ম্যাট্রিক্স। কিন্তু আমরা যেসব ভ্যারিয়েবল নিয়ে কাজ করছি দুইটাই কলাম ম্যাট্রিক্স। তাই গুণ করার জন্য একটা কলাম ম্যাট্রিক্সকে রো ম্যাট্রিক্সে কনভার্ট করে নিতে পারি। এই কনভার্শনের নাম হল Transpose করা। ট্রান্সপোজ করা খুবই সহজ, ম্যাট্রিক্সের রো গুলিকে কলাম আকারে সাজালে কিংবা কলামগুলোকে রো আকারে সাজালেই হবে।

আমাদের এখানে মডিফাই করতে হবে থিটা ম্যাট্রিক্সকে, সুতরাং

$$

\theta^{T} = \begin{bmatrix} \theta_{0} & \theta_{1} & \theta_{2} & \ldots & \theta_{n} \end{bmatrix}

$$

এখানে সুপারস্ক্রিপ্ট `T` দিয়ে ট্রান্সপোজ অপারেশন বুঝানো হয়েছে।

### হাইপোথিসিস ম্যাট্রিক্স নোটেশনে

$$

h_{0}(x) = \theta_{0}x_{0} + \theta_{1}x_{1} + \ldots + \theta_{n}x_{n}

\; = \theta^{T}X

$$

আশাকরি ভালমত বোরড হয়ে গেছেন, যাই হোক আর্টিফিশিয়াল ইন্টেলিজেন্স, ডেট সায়েন্স যেটাই হোক না কেন; লিনিয়ার অ্যালজেব্রা ছাড়া এক মূহুর্তও চলে না। ইমেজ প্রসেসিং শেখার সময়ও একগাদা ম্যাট্রিক্স বেজড ম্যাথ নিয়ে ঘাঁটাঘাঁটি করা লাগবে।

## মডিফাইড গ্রেডিয়েন্ট ডিসেন্ট

মাল্টিভ্যারিয়েবল রিগ্রেশনের ক্ষেত্রে গ্রেডিয়েন্ট ডিসেন্টের অ্যালগরিদমও পরিবর্তিত হবে।

আগের অ্যালগরিদমটা ছিল,

`repeat until convergence {`

$$\theta_{j} := \theta_{j} - \alpha \frac{\delta}{\delta \theta_{j}} J(\theta_{j})$$

`}`

যেখানে,

$$

\frac{\delta}{\delta \theta} J(\theta_{j}) = \frac{1}{m} \sum_{i=1}^{m} \left( h_{\theta} (x^{(i)} - y^{(i)}) \right)

$$

### যখন, $$ n = 1 $$

`Repeat`

`{`

$$

\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_{\theta} (x^{(i)}) - y^{(i)} \right)

$$

$$

\theta_{1} := \theta_{1} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_{\theta} (x^{(i)}) - y^{(i)} \right)x^{(i)}

$$

`}`

### পরিবর্তিত সূত্র, যখন $$ n \ge 1 $$

`Repeat {`

$$

\theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_{\theta} (x^{(i)}) - y^{(i)} \right)x^{(i)}_{j}

$$

`}`

যেহেতু, একাধিক ভ্যারিয়েবলের জন্য,

$$

\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_{\theta} (x^{(i)}) - y^{(i)} \right)x^{(i)}_{0}

$$

$$

\theta_{1} := \theta_{1} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_{\theta} (x^{(i)}) - y^{(i)} \right)x^{(i)}_{1}

$$

$$

\theta_{2} := \theta_{2} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_{\theta} (x^{(i)}) - y^{(i)} \right)x^{(i)}_{2}

$$

$$

\dots

$$

চলবে,

`}`

পরের পর্বে আমরা পাইথনে কোড লিখব।