# লিনিয়ার রিগ্রেশন : দ্বিতীয় পর্ব

আমরা গত পর্বে লিনিয়ার রিগ্রেশনের বেসিক জানার পাশাপাশি কস্ট ফাংশন ক্যালকুলেশন সম্পর্কে কিছুটা জেনেছিলাম। আজকে আমরা নিচের বিষয়গুলো সম্পর্কে জানার চেষ্টা করব।

## আজকের আলোচনার বিষয়বস্তু

* [ ] কস্ট ফাংশন ইনটুইশন - ২ 
	* $$ J(\theta) $$ এর গ্রাফ
* [ ] গ্রেডিয়েন্ট ডিসেন্ট (Gradient Descent) অপ্টিমাইজেশন 

 
# কস্ট ফাংশন ইনটুইশন
 
এতক্ষণে লিনিয়ার মডেল সম্পর্কে ভালই ধারণা হয়েছে আশা করি, সেটা যদি হয়ে থাকে আমরা আরেকবার ঘুরে আসি কস্ট ফাংশনের কাছ থেকে। 

## কস্ট ফাংশনের গ্রাফ দিয়ে লাভ কী? 
 
আমাদের কাজ ছিল কস্ট মিনিমাইজ করা। সকল ইঞ্জিনিয়ারিংয়ের মূল লক্ষ্য তাই। যত কম রিসোর্স ব্যবহার করে যত ভাল ফলাফল পাওয়া যায়। তেমনি মেশিন লার্নিংয়ের জন্য আমাদের মূল লক্ষ্য থাকবে কতটা নির্ভুল প্রেডিকশন করা যায়। 
 
আমরা যদি কতগুলো মডেলের কস্ট ফাংশন এর রেজাল্ট স্ক্যাটার প্লট করি তাহলে আমরা গ্রাফ থেকে সহজেই ট্র্যাক করতে পারব সবচেয়ে কম এরর কোন প্যারামিটারের জন্য। 
 
সবকিছু বাদ দিয়ে নতুন করে একটা জিনিস দেখা যাক, নিচের ডেটাসেট এর কথা চিন্তা করা করি, 
 
| আয় (X) | ব্যয় (Y) |
|----------|----------|
| 10   |  5 |
| 100 | 50 |
| 1000 | 500 |
 
#### গ্রাফ
 
এই ডেটাসেটের গ্রাফ এইরকম,
 
![graph](http://i.imgur.com/uQHG5GZ.png)
 
এটা প্রেডিক্ট করার  জন্য আমরা এই মডেল ব্যবহার করব : $$ h_{0}(\theta) = \theta \times X $$
 
বিভিন্ন $$ \theta $$ এর মানের জন্য আমরা $$ J(\theta) $$ প্লট করব। মানে প্রতি প্রেডিকশনে কস্ট ক্যালকুলেট করব। তারপর দেখব $$ \theta $$ এর কোন মানের জন্য $$ J(\theta) $$ এর মান সর্বনিম্ন আসে। 

# $$ h_{0}(\theta) = \theta \times X $$ সাপেক্ষে $$ J(\theta) $$ 

### ধরি $$ \theta = 0.1 $$ 

তাহলে প্লট আসবে এরকম, 

![hypo1](http://i.imgur.com/h72z5J1.png)

**কস্ট ক্যালকুলেশন:** $$J(0.1) = \frac{1}{2 \times 3} \times { 4^{2} + 40^{2} + 400^{2} } = 26936.0 $$

### আবার ধরি $$ \theta = 0.2 $$ 

তাহলে প্লট,

![hypo2](http://i.imgur.com/vPT9kEm.png)

**কস্ট ক্যালকুলেশন:** $$J(0.2) = \frac{1}{2 \times 3} \times { 3^{2} + 30^{2} + 300^{2} } = 15151.5 $$

### আবার ধরি $$ \theta = 0.3 $$

তাহলে প্লট, 

![hypo3](http://i.imgur.com/e5SFZMi.png)

**কস্ট ক্যালকুলেশন:** $$J(0.3) = \frac{1}{2 \times 3} \times { 2^{2} + 20^{2} + 200^{2} } = 6734.0 $$

### আবারও ধরি $$ \theta = 0.4 $$

![hypo4](http://i.imgur.com/vaVYjk5.png)

**কস্ট ক্যালকুলেশন:** $$J(0.4) = \frac{1}{2 \times 3} \times { 1^{2} + 10^{2} + 100^{2} } = 1683.5 $$

###  $$ \theta = 0.5 $$

![hypo5](http://i.imgur.com/PaLTv7n.png)

**কস্ট ক্যালকুলেশন:** $$J(0.5) = \frac{1}{2 \times 3} \times { 0^{2} + 0^{2} + 0^{2} } = 0 $$

### থিটা এর মান আরও বাড়ালে, $$ \theta = 0.6 $$

![hypo6](http://i.imgur.com/fOXlPgu.png)

**কস্ট ক্যালকুলেশন:** $$J(0.6) = \frac{1}{2 \times 3} \times { (-1)^{2} + (-10)^{2} + (-100)^{2} } = 1683.5 $$

### আরও বাড়িয়ে $$ \theta = 0.7 $$

![hypo7](http://i.imgur.com/FVKxNmT.png)

**কস্ট ক্যালকুলেশন:** $$J(0.7) = \frac{1}{2 \times 3} \times { (-2)^{2} + (-20)^{2} + (-200)^{2} } = 6734.0 $$

থাক আর বাড়ালাম না, এখন আমরা প্রতি থিটার মানের জন্য যতগুলো $$ J(\theta) $$ এর মান পেয়েছি সেগুলোর স্ক্যাটার প্লট তৈরি করি,

## কস্ট ফাংশন গ্রাফ

![costfunc](http://i.imgur.com/AiIKd42.png)

`Python 2 and 3`
```python
J = [26936.0, 15151.5, 6734.0, 1683.5, 0, 1683.5, 6734.0]
theta = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
colors = ['blue', 'black', 'orange', 'pink', 'magenta', 'brown', 'aqua']

for i in range(len(J)):
    lbl = 'Hypothesis H = %0.1f * x' % theta[i]
    plt.scatter(x[i], J[i], linewidth=5, color=colors[i], label=lbl)
    
plt.legend(loc='best')
plt.title('Cost Function Graph')
plt.xlabel('Theta')
plt.ylabel('J (theta)')
plt.show()
```

***

গ্রাফ থেকে কী বুঝলাম? $$ \theta = 0.5 $$ এর জন্য কস্ট সবেচেয়ে কম। মানে প্রেডিকশন সবেচেয়ে বেটার যখন থিটার মান $$ 0.5 $$। এভাবে প্রতিটা মডেলের কস্ট ফাংশন থেকে আমরা ধারণা করতে পারি মডেলের পার্ফর্মেন্স কতটা ভাল। 

## যদি আমাদের মডেল $$ J(\theta_{0}, \theta_{1}) = \theta_{0} + \theta_{1} \times x $$ হত

তাহলে সেটার প্লট হতে পারত এরকম,

![contourplot](http://i.imgur.com/XfigBKL.png)

আমরা অবশেষে কস্ট ফাংশন সম্পর্কে অনেক কিছু জানতে পারলাম। এখন আমরা দেখব Cost Function Minimization Using **Gradient Descent**।

# Gradient Descent অ্যালগরিদম

ক্যালকুলাস মনে আছে? ডিফারেনসিয়েশন? সেটাই আমাদের এখন কিছুটা কাজে আসবে। যদি মনে না থাকে তাহলে আগে একটু ডিফারেনসিয়েশন দেখা যাক। 

## Differentiation : Method for Calculating Slope at a specific point of  a function

কোন বিন্দুতে কোন ফাংশনের ডেরিভেটিভ মানে হল *সেই বিন্দুতে ওই ফাংশনের* **স্পর্শকের** ঢাল। ধরি, $$ y = f(x) $$ যেকোন একটি ফাংশন, এখন আমরা তার $$ (x_{1}, y_{1}) $$ বিন্দুতে যে স্পর্শক, তার ঢাল ($$ X $$ অক্ষের সাথে রেখাটি কত ডিগ্রি কোণ উৎপন্ন করে) জানতে চাই। তাহলে আমরা $$ f(x) $$ কে স্বাধীন চলক $$ x $$ এর সাপেক্ষে ডিফারেনসিয়েট করব। ডিফারেনসিয়েট অপারেটর টা লেখে এইভাবে $$ \frac{dy}{dx} $$ বা $$ \frac{df(x)}{dx} $$ ।

নিচের ছবিটা দেখা যাক, 

![diff](http://i.imgur.com/8gRQGuu.png)

## Slope বা ঢাল 

![slp](https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Znak_A-23.svg/272px-Znak_A-23.svg.png)

ঢালের সূত্র হচ্ছে , $$ m = \frac{\Delta y}{\Delta x} $$ 

ঢালের মান চার ধরণের, নন-জিরো পজিটিভ, নেগেটিভ, জিরো এবং অসংজ্ঞায়িত। এই মানের ভিত্তিতে আমরা ঢালকে ক্লাসিফাই করতে পারি। 

### এই ঢাল চার ভাগ করা যায়,
 
* **ধনাত্মক ঢাল (Positive Slope)**
	* যে ঢাল $$ X $$ অক্ষের সাথে সূক্ষ্মকোণ উৎপন্ন করে সেটাকে ধনাত্মক ঢাল বলে। ধনাত্মক ঢাল আসলে বলে তার দিকে গেলে $$ y $$ এর মান বাড়বে।
* **ঋণাত্মক ঢাল (Negative Slope)**
	* যে ঢাল $$ X $$ অক্ষের সাথে স্থূলকোণ উৎপন্ন করে সেটাকে ঋণাত্মক ঢাল বলে। ঋণাত্মক ঢাল বলে তার দিকে গেলে $$ y $$ এর মান কমবে।
* **শূন্য ঢাল (Zero Valued Slope)**
	* যে ঢাল $$ X $$ অক্ষের সাথে $$ 0  $$ ডিগ্রি কোণ উৎপন্ন করে সেটাকে শূন্য ঢাল বলে। 
* **অসংজ্ঞায়িত ঢাল (Undefined Slope)**
	* যে ঢাল $$ X $$ অক্ষের সাথে $$ 90 $$ ডিগ্রি উৎপন্ন করে সেটাকে ধনাত্মক ঢাল বলে। 

একনজরে ঢালগুলো,

![slopes](http://i.imgur.com/gjSismX.png)

***

###  Partial Derivative 

আমাদের মূলত কাজে লাগবে পার্শিয়াল ডেরিভেটিভ। একটা ফাংশন যে সব সময় একটা ভ্যারিয়েবলের উপর ডিপেন্ডেন্ট থাকবে সেটা সত্য নয়। যেমন: $$ z = f(x, y) = x^{2} + xy + y^{2} $$ এই ফাংশনটার কথাই চিন্তা করা যাক, এখানে $$ z $$ ভ্যারিয়েবলটি $$ x, y $$ দুইটার উপর নির্ভরশীল। তাই আমরা যদি $$ x $$ ও $$ y $$ দুইটার সাপেক্ষে $$ z $$ এর পরিবর্তন ট্র্যাক করতে চাই তাহলে একটা ডেরিভেটিভ দিয়ে হবে না। 

$$ z = x^{2} + xy + y^{2} $$

$$ \frac{\delta z}{\delta x} = 2x + y $$ যখন $$ y $$ ধ্রুবক 

$$ \frac{\delta z} {\delta y} = 2y + x $$ যখন $$ x $$ ধ্রুবক


আমরা যদি $$ \theta_{1} $$ প্যারামিটার দিয়ে কস্ট ফাংশন ক্যালকুলেট করি তাহলে আমাদের সাধারণ ডেরিভেটিভ নিলেই হচ্ছে, কিন্তু যদি $$\theta_{0}, \theta_{1} $$ দুই কিংবা তার বেশি প্যারামিটার বিশিষ্ট  কস্ট ফাংশন নেই তাহলে আমাদের অবশ্যই পার্শিয়াল ডেরিভেটিভ নিতে হবে। আপাতত আমরা এক প্যারামিটার বিশিষ্ট কস্ট ফাংশন দিয়ে গ্রেডিয়েন্ট ডিসেন্ট বোঝার চেষ্টা করব। 
	
প্রশ্ন আসতে পারে, এই ঢাল দিয়ে আমরা করব টা কী? আসলে ক্যালকুলাসের সামান্য(!) কনসেপ্ট দিয়ে আমরা বিলিওন বিলিওন সেকেন্ড বাঁচাতে পারি।  

আমরা ডিফারেনসিয়েশন ও ঢালের কনসেপ্ট দিয়ে কস্ট মিনিমাইজ করার চেষ্টা করব। আর সেই চেষ্টার জন্য আমরা যে অ্যালগরিদম ব্যবহার করব সেটাই Gradient Descent।

***

## গ্রেডিয়েন্ট ডিসেন্ট

### অ্যালগরিদম

```repeat until convergence {``` 

$$ \theta_{j} := \theta_{j} - \alpha \frac{\delta}{\delta \theta_{j}} J(\theta_{j}) $$

```}```

#### ম্যাথমেটিক্যাল নোটেশন

| মানে | ম্যাথ | প্রোগ্রামিং |
|-------|------|-----------|
| x ও y সমান  | x= y |  x == y    |
| y এর মান x এ অ্যাসাইন করা  |x := y | x  = y |
| x আপডেট উদাহরণ | x := x + 1 | x = x + 1 |


* তারমানে $$ := $$ এইটা দিয়ে বোঝানো হচ্ছে  $$ \theta_{j} $$ এর মান প্রতিবার আপডেট করতে হবে। 

* এখানে $$ \alpha $$ হল লার্নিং রেট (Learning Rate)

 ## গ্রেডিয়েন্ট ডিসেন্ট ইনটুইশন 
 
অ্যালগরিদম আসলে কী বলছে? আমরা আগেই জানি মেশিন লার্নিং মডেল ট্রেইনিং মানে হচ্ছে মডেলের ইন্টার্নাল প্যারামিটার গুলো এমন ভাবে সেট করা যাতে আমাদের প্রেডিকশন নির্ভুল হয়। আমরা কয়েকটা গ্রাফের মাধ্যমে বোঝার চেষ্টা করি আসলে গ্রেডিয়েন্ট ডিসেন্ট অ্যালগরিদমের কাজটা কী।
 
### ধরি আমাদের কস্ট ফাংশন $$ J(\theta_{1}) $$ 
 
এইবার যেকোন একটা $$\theta_{1} $$ এর মান ধরি, এবং সেই বিন্দুতে ডিফারেনসিয়েট করি। যদি ঢাল ধনাত্মক হয়, এর মানে ওইদিকে গেলে $$ J(\theta_{1}) $$ মান বাড়বে এবং উল্টা দিকে গেলে তার মান কমবে। নিচের ছবিটা দেখলেই বুঝা যাবে।

![graddescent1](http://i.imgur.com/6ag6NUd.png)

এইবার আমরা আরেকটা বিন্দু ধরি, যেটা কিনা লোকাল মিনিমাম এর বামে অবস্থান করে। 
 
![graddescent2](http://i.imgur.com/djAEVSQ.png)
 
অর্থাৎ গ্রেডিয়েন্ট ডিসেন্ট সূত্রটি বলছে আমাদের কোন দিকে গেলে কস্ট ফাংশনটা মিনিমাইজ হবে। এটা হল যখন একটা প্যারামিটার। এইরকম শত শত প্যারামিটারের সময় ভিজুয়ালাইজ করাটা সুবিধাজনক নয় তবে সব ক্ষেত্রে কাজটা ঠিক এইভাবেই হয়ে থাকে। 
 
এই আপডেট ততক্ষণ চলতে থাকে যতক্ষণ না মিনিমাম পয়েন্টে পৌঁছাবেন। মিনিমাম পয়েন্টে অ্যালগরিদমটি অটোমেটিক স্টপ হয়ে যাবে কারণ মিনিমাম পয়েন্টে $$ \frac{\delta J(\theta_{1})}{\delta \theta_{1}} = 0 $$ আর গ্রেডিয়েন্ট অংশ যদি $$ 0 $$ হয় তাহলে আপডেটের কিছু থাকবে না। 
 
এই পর্ব এই পর্যন্তই, পরবর্তী পর্বে আরেকদফা লিনিয়ার রিগ্রেশন, মাল্টি প্যারামিটারে গ্রেডিয়েন্ট ডিসেন্ট এবং ব্যাচ গ্রেডিয়েন্ট ডিসেন্ট সম্পর্কে জানতে পারব। 
 
# সচরাচর জিজ্ঞাস্য প্রশ্ন
 
### লার্নিং রেট কী? 

> লার্নিং রেট বা $$ \alpha $$ বলতে বুঝায় (ফিজিক্যাল মিনিং) কত দ্রুত কস্ট ফাংশন লোকাল মিনিমামে	কনভার্জ করতে চান। লার্নিং রেট কমালে $$ \theta_{1} $$ এর মান মিনিমামে কনভার্জ করতে সময় (ইটারেশন) বেশি নিবে মানে অনেকবার আপডেট হতে হবে। লার্নিং বাড়ালে আপডেট কম হবে। এই আলফা হতে হবে  যেকোন  পজিটিভ সংখ্যা।  

### লার্নিং রেট বাড়ালে বা কমালে কী ইফেক্ট সৃষ্টি হতে পারে?
 
> মনে করুন, আপনার চোখে পট্টি বেঁধে একটা উচুনিচু ভূমিতে ছেড়ে দেওয়া হল। এবং বলা হল, আপনার কাজ হবে সবচেয়ে নিচু জায়গাটা বের করা। এখন যদি আপনি বড় বড় স্টেপে হাঁটেন তাহলে মিনিমাম পয়েন্ট এড়িয়ে যেতে পারেন, আবার ছোট ছোট স্টেপে হাঁটলে নিচু জায়গা বের করতে অনেক সময় লাগবে। এই যে স্টেপ নিচ্ছেন সেটাকে আমরা লার্নিং রেটের অ্যানালজি বলতে পারি। 
 
 ![alphaeffect](http://i.imgur.com/UaBc5h6.png)

### স্টেপের সাথে সাথে লার্নিং রেট বাড়ানো/কমানোর দরকার আছে কী?

> না নেই, কারণ মিনিমাম লোকাল পয়েন্টের দিকে আগাতে থাকলে অটোমেটিক গ্রেডিয়েন্ট ডিসেন্ট অ্যালগরিদমের আপডেট স্টেপ কমে যায়। তাই $$ \alpha $$ এর মান যদি ফিক্সড থাকে তাহলেও সেটা মিনিমাম পয়েন্টে কনভার্জ করবে।

### $$ \theta_{1} $$ এর মান বা সর্বোপরি প্যারামিটারগুলোর মান শুরুতে র‍্যান্ডম নেওয়ার উদ্দেশ্য কী? 

> এই প্রশ্নের উত্তর অনেক বিশাল, র‍্যান্ডম পয়েন্টে প্যারামিটার ইনিশিয়ালাইজেশনের মূল সুবিধা হচ্ছে গ্লোবাল মিনিমাম বের করা। একই গ্রাফের লোকাল মিনিমাম বা গ্লোবাল মিনিমাম থাকতে পারে। লোকাল মিনিমাম বলতে সেই পয়েন্ট কে বোঝানো হয় যেটা সামগ্রিক গ্রাফের মধ্যে **তুলনামূলক** নিম্নবিন্দু। আর গ্লোবাল মিনিমাম হল পুরো গ্রাফের এমন একটা পয়েন্ট সেটাই সর্বনিম্ন বিন্দু। 


>  আবার আমরা চোখে পট্টির উদাহরণে ব্যাক করি। ধরুন আপনাকে হেলিকপ্টারে করে এই পয়েন্টে ছেড়ে দিয়ে মিনিমাম পয়েন্ট বের করতে বলা হল। আপনি সোজা যেতে থাকলেন এবং লোকাল মিনিমাম বের করলেন। এখন যদি আপনাকে বার বার ওই পয়েন্টেই ছাড়ি এবং আপনি সোজাই যেতে থাকেন আপনি প্রত্যেকটা বার লোকাল মিনিমাম পয়েন্ট পেয়ে লাফালাফি শুরু করে দেবেন। 

![localmin](http://i.imgur.com/p1eBseQ.png)

> এবার আপনাকে র‍্যান্ডমলি হেলিকপ্টার থেকে এই বিন্দুতে ছাড়া হল এবং এইবার আপনি আসলেই গ্লোবাল পয়েন্টে যেতে পারবেন।

![globalmin](http://i.imgur.com/eBXnoz5.png)
